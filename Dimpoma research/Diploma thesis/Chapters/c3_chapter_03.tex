\chapter{Проведення чисельного експерименту}
\label{chap: practice}

Для імплементації алгоритмів розв’язування задачі побудови оцінок невідомих параметрів моделі було використано засоби мови програмування \texttt{Python} версії \texttt{3.8.10} в інтегрованому середовищі розробки \texttt{Visual Studio Code} версії \texttt{1.78.2}.

Вибір мови програмування зумовлювався широким арсеналом вбудованих програмних пакетів мови \texttt{Python} для роботи з масивами даних та математичними обчисленнями (бібліотеки \texttt{NumPy}, \texttt{itertools}, \texttt{SciPy}, \texttt{random}, \texttt{numda}), а також наявними інструментами для візуалізації даних (пакети \texttt{pandas}, \texttt{matplotlib}). Додаток Б містить тексти ключових блоків коду, необхідних для проведення чисельного експерименту.

Для ефективного керування великою кількістю взаємопов'язаних програмних блоків (функцій), а також для більш наочної демонстрації отриманих результатів було розроблено графічний інтерфейс користувача засобами пакета \texttt{PySimpleGUI} мови \texttt{Python}. Додаток А містить опис та приклад роботи розробленого програмного модуля.

\section{Моделювання об'єкта дослідження}
\label{chap: modeling}

Для експериментальної перевірки ефективності використаних методів необхідно згенерувати набір бінарних послідовностей довжини $N$ із заданим параметром $p$, при цьому еволюція ланцюга має відбуватися згідно з узагальненою моделлю Еренфестів.

Тож нехай $x^1 \in \{ 0,1 \}^N$~--- детермінований або обраний навмання на множині станів $E$ початковий стан. Тоді подальше генерування ланцюга від стану $x^t$ до стану $x^{t+1}$ протягом $T$ моментів часу при заданому параметрі $p$ відбуватиметься за таким алгоритмом:

\newpage
\begin{enumerate}
    \item обрати навмання для стану $x^t$ індекс $j \in \{ 1,\,2,\ldots,\,N \}$;
    \item згенерувати $u^t \sim U(0,1)$~--- випадкову величину $u^t$ з рівномірного на відрізку $(0,1)$ розподілу;
    \item розглянути дві альтернативи:
    \begin{enumerate} 
        \item якщо $u^t \leqslant p$, то всі елементи наступного стану $x^{t+1}$ покласти рівними елементам поточного стану $x^{t}:$
        \begin{equation*}
            \forall i=\overline{1,N}\ :\ x^{t+1}_i=x^{t}_i
        \end{equation*}
        \item якщо $u^t > p$, то елемент $x^{t+1}_j$ наступного стану $x^{t+1}$ покласти протилежним до елементу $x^{t}_j$ бінарним символом, а решту значень вважати рівними елементам поточного стану $x^{t}:$
        \begin{gather*}
            \forall i \neq j\ :\ x^{t+1}_i=x^{t}_i \\
            x^{t+1}_j=1 - x^{t}_j
        \end{gather*}
    \end{enumerate}
    \item повторити кроки 1) -- 3) до формування ланцюга довжиною $T$.
\end{enumerate}

Таким чином, у рамках чисельного експерименту було згенеровано прихований ланцюг Маркова протягом $T=200$ моментів часу для бінарних послідовностей довжини $N=5$ при заданому параметрі моделі $p=0.2$, починаючи з початкового стану $x^1=(0,0,0,1,0)$. 

Значення спостережень формувалися як суми значень елементів прихованого стану по таких неперетинних множинах індексів:
\begin{equation}\label{eq: example observed indexes}
    I=\{I_1,\,I_2\}=\{(2,3),\,(1,4)\}
\end{equation} 

Отже, задано всі необхідні ввідні дані для того, щоб визначити приховану марковську модель $\lambda=\left( \pi,\,A,\,B \right):$ початковий розподіл $\pi$~\eqref{eq: pi initial distribution}, матрицю перехідних імовірностей~\eqref{eq: A transition matrix} та матрицю $B$~\eqref{eq: B emission matrix}.

\section{Задача навчання}
\label{chap: learning}

За наявними спостереженнями про динаміку набору функціоналів від станів прихованого ланцюга бінарних послідовностей оцінимо керуючий параметр системи та порівняємо отриману величину з істинним значенням параметра.

\subsection{Ітераційний алгоритм Баума-Велша}

Використовуючи формулу переоцінки~\eqref{eq: p baum-welch estimation}, продемонструємо на Рис.~\ref{pic: p baum-welch estimation} збіжність алгоритму Баума-Велша при оцінці параметра $p$, починаючи з наближення $p^{(0)}=0.55$.

\begin{figure}[H]\centering
    \setfontsize{14pt}
    \input{Tikzplots/p baum-welch estimation.tikz}
    \caption{Ітерації алгоритму Баума-Велша при оцінці параметра $p$}
    \label{pic: p baum-welch estimation}
\end{figure}

За $n=12$ ітерацій алгоритм досягає точності $\varepsilon=0.0001$ переоцінки оцінюваного параметра. При цьому, отримане значення $p^{(12)}=0.1959$ відрізняється від свого істинного значення $p=0.2$ на величину $\delta=0.0041$.

\subsection{Побудова оцінки методами математичної статистики}

Застосуємо інший підхід для обчислення значення керуючого параметра моделі за наявними спостереженнями: знайдемо точкову оцінку $\widehat{\,p\,}$ згідно з виразом~\eqref{eq: p statistical estimation}.

Отримана статистична оцінка відрізняється від ітераційної оцінки на величину, що складає менш ніж $\delta=p^{(12)}-\widehat{\,p\,}=1.185 \cdot 10^{-6}$. Інакше кажучи, для досліджуваної прихованої марковської моделі оцінка за ітераційним алгоритмом збігається до значення змістовної статистичної оцінки, яка, своєю чергою, є безпосереднім відгуком спостережуваних даних. Окреслений взаємозв'язок проілюстровано на рисунку нижче:

\begin{figure}[H]\centering
    \setfontsize{14pt}
    \input{Tikzplots/p statistical estimation.tikz}
    \caption{Збіжність оцінки ітераційного алгоритму до значення змістовної статистичної оцінки}
    \label{pic: p statistical estimation}
\end{figure}

Отже, при збільшенні довжини $T$ спостережуваного ланцюга ймовірність відхилення оцінки за алгоритмом Баума-Велша від істинного значення параметра буде прямувати до нуля.

\subsection{Висновки щодо точності оцінювання}
\label{chap: restarts}

Оскільки ітераційний алгоритм Баума-Велша є збіжним лише до точки локального максимуму функції правдоподібності~\cite[розділ 15]{Koski2001}, виконаємо висновки щодо ефективності використаних методів для розв'язку задачі навчання шляхом аналізу результатів $R=200$ незалежних запусків алгоритму оцінювання невідомого параметра.

А саме: щоразу генеруватимемо ланцюг Маркова з параметром $p=0.2$ і множиною спостережуваних індексів~\eqref{eq: example observed indexes} з навмання обраного стану $x^1 \in E$. Крім того, розпочинатимемо черговий ітераційний процес переоцінки з наближення $p^{(0)} \in (0.05,\,0.95)$, обраного навмання.

В результаті було отримано вибірку оцінок $\left\{ p^{[r]} \right\}_{r=\overline{1,R}}$, гістограма яких наведена нижче:

\begin{figure}[H]\centering
    \setfontsize{14pt}
    \input{Tikzplots/p restarts baum-welch estimations.tikz}
    \caption{Гістограма значень оцінок $R=200$ незалежних рестартів алгоритму Баума-Велша}
    \label{pic: p restarts baum-welch estimations}
\end{figure}

Вибіркове середнє сформованої вибірки складає
\begin{equation*}
    \mu_p = \frac{1}{R} \sum\limits_{r=1}^R p^{[r]} = 0.2004,
\end{equation*}
а вибіркова дисперсія
\begin{equation}\label{eq: p sample covariance}
    S^2_p = \frac{1}{R-1} \sum\limits_{r=1}^R \left( p^{[r]} - \mu_p \right)^2 = 0.0019
\end{equation}

Отже, ітераційний алгоритм Баума-Велша продемонстрував високу точність оцінювання:
\begin{equation}\label{eq: p accuracy}
    \delta_p = \left|\, p-\mu_p\, \right| = \left|\, 0.2-0.2004\, \right| = 0.0004
\end{equation}

\section{Задача декодування}
\label{chap: decoding}

Відтворимо ланцюг бінарних послідовностей згенерованої у розділі \ref{chap: modeling} моделі шляхом застосування алгоритму декодування Вітербі. Для цього використаємо оцінене значення керуючого параметра $p^{(12)} \approx \widehat{\,p\,}=0.1959$, отримане через формулу переоцінки~\eqref{eq: p baum-welch estimation} або шляхом побудови точкової оцінки~\eqref{eq: p statistical estimation}. 

Якість отриманих результатів схарактеризуємо через порівняння в кожен момент часу $t$ істинної прихованої бінарної послідовності $X^t$ та декодованої $\widehat{X}^t$ за допомогою відстані Геммінга:
\begin{equation*}
    d_H\left( X^t,\,\widehat{X}^t \right) = \sum_{i=1}^{N} \mathbbm{1}\left( X^t_i \neq \widehat{X}^t_i \right)
\end{equation*} 

Таким чином, чим більше символів між справжнім та декодованим станами збігатимуться, тим меншою буде відповідна відстань Геммінга. 

\begin{figure}[H]\centering
    \setfontsize{14pt}
    \input{Tikzplots/decoding task.tikz}
    \caption{Результати алгоритму декодування Вітербі}
    \label{pic: viterbi decoding algorithm}
\end{figure}

З гістограми результатів (Рис.~\ref{pic: viterbi decoding algorithm}) видно, що $17\%$ усього ланцюга декодовано правильно. Наявність близько $40\%$ помилок в одному символі може бути наслідком того, що одного елемента стану немає серед спостережуваних областей~\eqref{eq: example observed indexes} ланцюга. Крім того, оцінка керуючого параметра має похибку $\delta=0.0041$ відносно свого істинного значення, що також впливає на результати задачі декодування.

Наступним кроком для кожного незалежного рестарту, проведеного у розділі~\ref{chap: restarts} (Рис.~\ref{pic: p restarts baum-welch estimations}), було виконано алгоритм декодування прихованих станів та отримано $R=200$ гістограм виду Рис.~\ref{pic: viterbi decoding algorithm}. Отримані розподіли значень для кожної з відстаней Геммінга наведені у Табл.~\ref{table: Hamming distributions}.

\begin{table}[H]\centering
    \setfontsize{14pt}
    \caption{Розподіли відстаней Геммінга для $R=200$ рестартів алгоритму декодування}
    \begin{tblr}{
            hlines,vlines,
            colspec={ccccccc},
            row{1-3}={mode=math},
        }
                  & d_H=0 & d_H=1 & d_H=2 & d_H=3 & d_H=4 & d_H=5 \\
        \mu_{d_H} & 28.25 & 28.02 & 18.6  & 18.95 & 3.04  & 3.16  \\
        S^2_{d_H} & 36.55 & 35.67 & 17.65 & 21.16 & 2.68  & 2.86  \\
    \end{tblr}
    \label{table: Hamming distributions}
\end{table} 

Як підсумок, точність результатів помітно варіюється від рестарту до рестарту. Причому закономірності вищої ефективності алгоритму декодування при точнішому значенні оцінки керуючого параметра як відносно свого істинного значення, так і відносно своєї змістовної точкової оцінки не прослідковується. 

\section{Задача локалізації}

Значення додаткових спостережуваних <<сигналів>> було згенеровано як суми елементів прихованого стану по множині індексів $I_* = (1,3,5)$. Маючи з розділів~\ref{chap: learning} та~\ref{chap: decoding} оцінку керуючого параметра та найбільш імовірний відтворений ланцюг бінарних послідовностей, віднайдемо набір елементів множини $I_*$.

Перш за все, з Табл.~\ref{table: dependence between |I_*| and T} робимо висновок, що кількості спостережень $T=200$ недостатньо для отримання точної оцінки потужності множини неявних індексів за формулою~\eqref{eq: ||I*|| estimation}.

\begin{table}[H]\centering
    \setfontsize{14pt}
    \caption{Залежність значення змістовної оцінки потужності $\widehat{|I_*|}$ від довжини ланцюга $T$}
    \begin{tblr}{
            hlines,vlines,
            colspec={cccccc},
            row{1-3}={mode=math},
        }
        T               & 200    & 400    & 600    & 800    & 1000    \\
        \widehat{\,p\,} & 0.1959 & 0.1823 & 0.1882 & 0.2099 & 0.2092  \\
        \widehat{|I_*|} & 2      & 2      & 2      & 3      & 3       \\
    \end{tblr}
    \label{table: dependence between |I_*| and T}
\end{table} 

Однак, оскільки довжина бінарних послідовностей $N$ є невеликою, для оцінки потужності шуканої множини можна скористатися емпіричною формулою вигляду
\begin{equation*}
    \widehat{|I_*|}=\max\limits_{1\leqslant t \leqslant 200} y^t_{I_*} = 3
\end{equation*}

\newpage
Застосуємо отримане значення до виразу~\eqref{eq: I^ estimation}, щоб віднайти елементи, які безпосередньо входять в $I_*:$ квадратична відстань~\eqref{eq: square average distance} вказуватиме на сукупність індексів $\widehat{I\,}_S=(1,2,5)$, а зважена відстань Жаккара~\eqref{eq: weighted Jaccard distance}~--- на сукупність $\widehat{I\,}_J=(1,2,3)$.

Дилему можна вирішити шляхом збільшення $T$ та подальшого використання змістовної оцінки~\eqref{eq: ||I* cup H|| estimation} для визначення взаємного розташування елементів множини неявних індексів відносно спостережуваних індексів~\eqref{eq: example observed indexes}.

\section{Задача навчання за спотвореними спостереженнями}

Для кожної зі спостережуваних областей~\eqref{eq: example observed indexes} змодельованого у розділі~\ref{chap: modeling} ланцюга $x^1,\ldots,\,x^T$ було обрано такі ймовірності спотворення: $q = (q_1,\,q_2) = (0.05,\,0.1)$. 

Окреслимо алгоритм утворення зашумленого ланцюга $\widetilde{x}^1,\ldots,\,\widetilde{x}^T$ із заданими коефіцієнтами $q_1$ та $q_2:$

\begin{enumerate}
    \item для кожного з індесів $i \in I_k,\ k=\overline{1,L}$ поточного стану $x^t$ згенерувати рівномірно розподілену випадкову величину $u_i \sim U(0,1):$
    \begin{enumerate}
        \item якщо $u_i \leqslant q_k$, то покласти
        \begin{equation*}
            \widetilde{x}^{t}_i=1 - x^{t}_i
        \end{equation*} 
        \item якщо $u_i > q_k$, то покласти
        \begin{equation*}
            \widetilde{x}^{t}_i=x^{t}_i
        \end{equation*} 
    \end{enumerate}
    \item для тих індесів $i \notin \bigcup\limits_{k=1}^L I_k$, які не належать жодній із множин спостережуваних індексів покласти
    \begin{equation*}
        \widetilde{x}^{t}_i=x^{t}_i
    \end{equation*}
    \item повторити кроки 1) -- 2) для кожного стану $x^t,\ t=\overline{1,T}$.
\end{enumerate}

Таким чином, маємо змогу визначити приховану марковську модель $\lambda=\left( \pi,\,A,\,B^q \right)$ з початковим розподілом $\pi$~\eqref{eq: pi initial distribution}, матрицею перехідних імовірностей~\eqref{eq: A transition matrix} та матрицею умовних імовірностей спостережень при заданих прихованих станах $B^q$~\eqref{eq: B distorted emission matrix}.

\subsection{Ітераційний алгоритм Баума-Велша}

Використовуючи формули переоцінки~\eqref{eq: distortion p estimation} та~\eqref{eq: distortion q estimation}, оцінимо керуючий параметр системи та ймовірності викривлення за наявними спотвореними спостереженнями про динаміку набору функціоналів від станів прихованого ланцюга бінарних послідовностей. 

Рис.~\ref{pic: p distortion estimation} та Рис.~\ref{pic: q distortion estimation} демонструють результати ітераційної переоцінки невідомих параметрів моделі при початкових наближеннях $p^{(0)}=0.55$ й $q^{(0)}=(0.3,\,0.4)$. 

\begin{figure}[H]\centering
    \setfontsize{14pt}
    \input{Tikzplots/p distortion estimation.tikz}
    \caption{Ітерації алгоритму Баума-Велша при оцінці \\ параметра $p$, враховуючи спотвореність спостережень}
    \label{pic: p distortion estimation}
\end{figure}

\begin{figure}[H]\centering
    \setfontsize{14pt}
    \input{Tikzplots/q distortion estimation.tikz}
    \caption{Ітерації алгоритму Баума-Велша при оцінці \\ компонент вектора $q$}
    \label{pic: q distortion estimation}
\end{figure}

Для досягнення точності переоцінки $\varepsilon=0.0001$ оцінюваного параметра $p$ у випадку спотворених даних знадобилося $n=53$ ітерацій. Отримане значення $p^{(53)}=0.2559$ відрізняється від свого істинного значення $p=0.2$ на показник $\delta=0.0559$. Водночас точність оцінки коефіцієнтів спотворення $q^{(53)} = \left( q^{(53)}_1,\,q^{(53)}_2 \right) = (0.0454,\,0.1184)$ складає $\delta=(\delta_1,\,\delta_2)=(0.0046,\,0.0184)$.

\subsection{Висновки щодо точності оцінювання}

Знову ж таки, оскільки ітераційний алгоритм Баума-Велша є збіжним лише до точки локального максимуму функції правдоподібності, виконаємо висновки щодо ефективності використаних методів для розв'язку задачі навчання шляхом аналізу результатів $R=200$ незалежних запусків ітераційного алгоритму.

\newpage
Проілюструємо гістограму вибірки оцінок $\left\{ p^{[r]} \right\}_{r=\overline{1,R}}$ на Рис.~\ref{pic: p distortion restarts estimations}.

\begin{figure}[H]\centering
    \setfontsize{14pt}
    \input{Tikzplots/p distirtion restarts estimations.tikz}
    \caption{Гістограма значень оцінок $R=200$ незалежних рестартів алгоритму Баума-Велша при спотвореності спостережень}
    \label{pic: p distortion restarts estimations}
\end{figure}

Вибіркове середнє вказаної вибірки складає
\begin{equation*}
    \mu^{\times}_p = \frac{1}{R} \sum\limits_{r=1}^R p^{[r]} = 0.2082,
\end{equation*}
а вибіркова дисперсія
\begin{equation*}
    \left( S^{\times}_p \right)^2 = \frac{1}{R-1} \sum\limits_{r=1}^R \left( p^{[r]} - \mu^{\times}_p \right)^2 = 0.0065
\end{equation*}

У порівнянні з показниками~\eqref{eq: p accuracy} та~\eqref{eq: p sample covariance} помітна втрата точності ітераційного алгоритму Баума-Велша при оцінці керуючого параметра $p$ за спотвореними спостереженнями:
\begin{equation*}
    \delta^{\times}_p = \left|\, p-\mu^{\times}_p\, \right| = \left|\, 0.2-0.2082\, \right| = 0.0082
\end{equation*}

\newpage
Частотна характеристика отриманих значень оцінок коефіцієнтів спотворення $\left\{ \left( q^{[r]}_1,\,q^{[r]}_2 \right) \right\}_{r=\overline{1,R}}$ зображена на тепловій карті Рис.~\ref{pic: q distirtion restarts estimations}. 

\begin{figure}[H]\centering
    \setfontsize{14pt}
    \input{Tikzplots/q distirtion restarts estimations.tikz}
    \caption{Гістограма значень оцінок ймовірностей спотворення у $R=200$ незалежних рестартах алгоритму Баума-Велша}
    \label{pic: q distirtion restarts estimations}
\end{figure}

Вибіркові середні складають
\begin{gather*}
    \mu_{q_1} = \frac{1}{R} \sum\limits_{r=1}^R q^{[r]}_1 = 0.0519 \\
    \mu_{q_2} = \frac{1}{R} \sum\limits_{r=1}^R q^{[r]}_2 = 0.1051 
\end{gather*}

В той час як вибіркові дисперсії
\begin{gather*}
    S^2_{q_1} = \frac{1}{R-1} \sum\limits_{r=1}^R \left( q^{[r]}_1 - \mu_{q_1} \right)^2 = 0.0004 \\
    S^2_{q_2} = \frac{1}{R-1} \sum\limits_{r=1}^R \left( q^{[r]}_2 - \mu_{q_2} \right)^2 = 0.0008 
\end{gather*}

Вказані показники свідчать про високу точність результатів:
\begin{align*}
    &\delta_{q_1} = \left|\, q_1-\mu_{q_1}\, \right| = \left|\, 0.05-0.0519\, \right| = 0.0019 \\
    &\delta_{q_2} = \left|\, q_2-\mu_{q_2}\, \right| = \left|\, 0.1-0.1051\, \right| = 0.0051
\end{align*}

\chapconclude{\ref{chap: practice}}

Для перевірки ефективності використаних методів оцінки параметрів частково спостережуваного ланцюга Маркова на бінарних послідовностях було проведено чисельний експеримент, у якому можна виділити такі етапи: генерування об'єкта дослідження із заданими параметрами; імплементація алгоритмів оцінювання шуканих параметрів моделі за частковими спостереженнями від згенерованого об'єкта; порівняльний аналіз отриманих оцінок з їхніми істинними значеннями.

Через особливості алгоритму Баума-Велша висновки щодо точності результатів виконувалися за статистичною обробкою багатьох незалежних перезапусків ітераційного алгоритму навчання.

У підсумку результати чисельного експерименту продемонстрували ефективність використаних методів, зокрема збіжність побудованих оцінок до істинних значень параметрів при збільшенні кількості спостережень.